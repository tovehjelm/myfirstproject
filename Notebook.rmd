---
title: "R basics"
output: word_document
---

# 1.1 About R
- R came from S
- R works in memory
    + Microsoft has out-of-memory processing capabilities 
    + There's some other potential options
- R is quirky AF
- R is case sensitive
- There's a package for that!

# 1.2 R packages
We can install from CRAN
```{r eval=FALSE}
install.packages("tidyverse")
```

We can make packages available to us using `library()` function.

```{r}
library("tidyverse")
```

```{r}
tidyverse_packages()

```

# 1.3 Vectors
1 dimensional objects

```{r}
colors <- c("red","blue","green")
colors2 = c("orange","purple")

nums <- 1:10
nums

about <- c(firstname="Tove",lastname="Hjelm")
about
```
Extracting values uses a grid system

```{r}
colors[1]
colors[-1]
colors[c(1,3)]
colors[1:2]
colors[-c(1,2)]

```
# 2.1 Tidyverse

To clean and prepare data from different sources.

```{r}
library(tidyverse)

# Change "data" to where your files are.
# Remove the col_types bit if your columns
# are fairly type safe.
list.files("data", full.names = TRUE) %>%
  map_df(read_csv, col_types = cols("c")) %>% 
  bind_rows() %>% 
  nrow()
```

# 2.2 Caret package
Create datasets with training data (features) and training outcome (what we want to predict)
Create datasets to test the model. Test data with features that are used to predict, and test outcome that has the actual cylender of the data we test.
method = what model we are using, neural network, regression etcetera. 
preprocess = olika typer av forbearbetning. pca = vilka variabler som ar viktiga, vilka faktorer som paverkar cylinderantal.

```{r}
library(caret)

## Loading required package: lattice

## 
## Attaching package: 'caret'

## The following object is masked from 'package:purrr':
## 
##     lift
## To predict cylinder (column 2) from other features.


training_data    <- mtcars[1:20,-2]
training_outcome <- mtcars[1:20,2]
test_data        <- mtcars[21:32,-2]
test_outcome     <- mtcars[21:32,2]

model <- train(training_data, training_outcome,
               method="lm",
               preProcess = c("scale","center","pca"))
predictions <- predict(model, test_data)
predictions
```
#2.3 Visualizations

```{r}
library(ggplot2)
library(datasauRus)

myPlot<- ggplot(datasaurus_dozen, aes(x,y)) +
           geom_point()
myPlot
```
```{r}
myPlot + facet_wrap(~dataset)
```
Uses the myPlot we defined but with the simpsons_paradox dataset instead of datasaurus_dozen. Makes it possible to reuse models with other data. 

(Simpsonâ€™s Paradox is the phenomenon where high level statistics provide one conclusion but evaluating sub-groups within the data provides a very different conclusion.)

```{r}
myPlot %+% simpsons_paradox
```
# 32. Visualizations, Common charts

```{r}
library(ggplot2)
library(datasauRus)
library(cowplot)

ggplot(datasaurus_dozen, aes(x = x, y = y)) + geom_point()

p <- ggplot(datasaurus_dozen, aes(x = x, y = y, color=dataset)) + geom_line() + facet_wrap(~dataset)
p

ggplot(data = mtcars, aes(x = rownames(mtcars), y = mpg)) + geom_col() + theme(axis.text.x = element_text(angle = 90))

plot1 <- ggplot(data = simpsons_paradox, aes(x = x, y = y)) + geom_point()
plot2 <- ggplot(data = mtcars, aes(x = am)) + geom_bar()
ggplot(data = mtcars, aes(x = rownames(mtcars), y = mpg)) + geom_col()
ggplot(data = simpsons_paradox, aes(x = x, y = y, colour = dataset)) + geom_jitter(width = 5)
ggplot(data = simpsons_paradox, aes(x = x, y = y, colour = dataset)) + geom_point(alpha = 0.5)
ggplot(data = simpsons_paradox, aes(x = x, y = y, size = dataset, colour = dataset, 
    shape = dataset, alpha = dataset)) + geom_point()
ggplot(data = simpsons_paradox, aes(x = x, y = y, group = dataset, colour = dataset)) + 
    geom_line()


plot_grid(plot1,plot2)

```

## Exercises
```{r}

#Iris
ggplot(data = iris, aes(x = Sepal.Width, y = Sepal.Length, color = Species, size = Petal.Width+Petal.Length)) + geom_point(alpha = 0.8)

#Barchart, get max for all y values
datasaurus_dozen %>%
  group_by(dataset) %>%
  summarise(y=max(y)) %>%
  ggplot(aes(x=dataset, y=y)) +
  geom_col()


```
## 33.Visualizations, Density charts

```{r}
ggplot(data = mtcars, aes(x = wt)) + geom_histogram()
ggplot(data = mtcars, aes(x = wt)) + geom_density()
ggplot(data = mtcars, aes(x = as.factor(am), y = wt)) + geom_violin()
ggplot(data = mtcars, aes(x = wt, mpg)) + geom_bin2d()
```

## 34. Faceting
```{r}
ggplot(data = datasaurus_dozen, aes(x = x, y = y)) + geom_point() + facet_wrap(~dataset)

ggplot(data = datasaurus_dozen, aes(x = x, y = y)) + geom_point() + facet_wrap(~dataset, 
    nrow = 5)
```
# Exercises
Create a facet scatterplot for simpsons_paradox

```{r}
library(ggthemes)
ggplot(data = simpsons_paradox, aes(x = x, y = y)) + geom_point() + facet_wrap(~dataset) + theme_classic()
```
## Packages

CRAN - cran.r-project.org, task views
Bioconductor
Follow:
Mara Averick on twitter for good uses of packages

## Follow

Mara Averick on twitter
Rladies
Jumpingrivers
R-bloggers
Setosa.io

#Packages that are good for Data Exploration


install.packages("devtools")
devtools::install_github("krupanss/IEDA")
# skimr::skim(dataset) to get a good overview of a dataset
# GGally::ggpair(dataset) to pair all variables with eachother
# Simmer - for distinct simmulations
# GGanimate - to plot changes over time
# DataExplorer
# rsample - to do samples

## Clean Ecommerce-data (Description contains a stringvalue with \x, R does not like that)

```{r}
ecommerce %>%
  mutate(Description=str_remove_all(Description,"[^[:alnum:]]")) %>% 
  skimr::skim()

```

```{r}
ecommerce %>% 
  ggplot(aes(x = Quantity)) + geom_density()
```
## Create awesome data explorer data
```{r}
#library(DataExplorer)
#DataExplorer::create_report(dataset, "filename.html")
```

## Distribution and simulaton

#Simulation one 
With set variables

```{r}
#dnorm(dataset) - Density distribution
#rnorm(n) - n random numbers from the distribution, default mean = 0, default sd = 1
library(tidyverse)
library(foreach)

run_simulations <- 100
nrows <- 5000
sample_p <- 0.7
dist_mean <- 0
dist_sd <- 1
p_threshold <- 0.05

sim_results <- foreach(runs=seq_len(run_simulations), .combine = c) %do% #for all of the following, do this, in sequence
  {
  
  outcome_var <- rnorm(nrows, mean = dist_mean, sd= dist_sd) #normally distributed sample with 5000 observations

use_for_training <- sample(1:nrows, nrows*sample_p) #sample with 5000*0.7 random numbers between 1 and 5000.

training <- outcome_var[use_for_training] #choose for training from the normal distribution in my_var the numbers that are in rows_in_training
testing <- outcome_var[-use_for_training] #choose for testing from the normal dstribution in my_var the numbers that are NOT in rows_in_training (the other 0.3 percent)

sim_ttest <- t.test(training, testing) 

ifelse(sim_ttest$p.value > p_threshold,
       "Same distribution, yay!",
       "Oh no, not the same distribution")
#Hypothesis - the two samples are NOT from the same distribution, means are not the same, there is a difference
#Null hypothesis - the two samples are from the same distribution, means are the same, there is no difference

}

sim_results %>% 
  fct_count() %>% 
  mutate(prop=scales::percent(n/sum(n)))
#gives a table of the counts of the times the result was "Means the same!" and the times it was "Means not the same!"

```

#Simulation two
With different values for the variables, to see how different variables effect the result - with just one simulation to see that it works. More iterations in the next step. 

```{r}

library(tidyverse)
library(foreach)

r_simulations <- seq_len(1)
nrows <- seq(5000, 200000, by = 10000) #sequence from 5000-200000, add 10000 each iteration.
sample_p <- c(.7, .8, .9)
dist_mean <- 0
dist_sd <- 1:10
p_threshold <- c(0.05, 0.1) #for p-hacking

#To create all combinations of factors that impact the outcome: 
combos<-expand.grid( 
  r=r_simulations, 
  n=nrows,
  s=sample_p,
  m=dist_mean,
  sd=dist_sd,
  p=p_threshold)
nrow(combos)

sim_results <- foreach(t=1:nrow(combos), 
                       .combine=rbind) %do% {
  df <- combos[t,]
  outcome_var <- rnorm(df$n, mean = df$m, sd=df$sd) 
  use_for_training <- sample(1:df$n, df$n*df$s) 
  training <- outcome_var[use_for_training] 
  testing <- outcome_var[-use_for_training] 
  sim_ttest <- t.test(training, testing) 
  df$result <- ifelse(sim_ttest$p.value > df$p,
       "Same distribution",
       "Not the same distribution")
df

}

sim_results %>% 
  count(result) %>% 
  mutate(prop=scales::percent(nn/sum(nn)))


```

#Simulation three
With parallel processing to be able to run multiple sequences at the same time

```{r}
library(tidyverse)
library(foreach)
library(parallel)
library(doParallel)
my_machine <- makeCluster(detectCores())
registerDoParallel(my_machine) #Use all four cores on the computer (more cores running on server)

r_simulations <- seq_len(100)
nrows <- seq(5000, 200000, by = 10000) 
sample_p <- c(.7, .8, .9)
dist_mean <- 0
dist_sd <- 1:10
p_threshold <- c(0.05, 0.1) 

combos<-expand.grid(
  r=r_simulations, 
  n=nrows,
  s=sample_p,
  m=dist_mean,
  sd=dist_sd,
  p=p_threshold)
nrow(combos)

sim_results <- foreach(t=1:nrow(combos), 
                       .combine=rbind) %dopar% #do in parallel
  {
  df <- combos[t,]
  outcome_var <- rnorm(df$n, mean = df$m, sd=df$sd) 
  use_for_training <- sample(1:df$n, df$n*df$s) 
  training <- outcome_var[use_for_training] 
  testing <- outcome_var[-use_for_training] 
  sim_ttest <- t.test(training, testing) 
  df$result <- ifelse(sim_ttest$p.value > df$p,
       "Same distribution",
       "Not the same distribution")
df

}

sim_results %>% 
  count(result) %>% 
  mutate(prop=scales::percent(nn/sum(nn)))

library(ggplot2)
sim_results %>% 
  mutate(sim=row_number()) %>% 
  gather(lever, value, -r, -sim, -result) %>% 
  count(lever, value, result) %>% 
  filter(result!="Same distribution") %>% 
  filter(lever!="m") %>% 
  ggplot(aes(x=value, y=n)) +
  geom_line(color="blue") + 
  facet_wrap(~lever, scales = "free") +
  theme_minimal() +
  geom_smooth()
  
```

##Predictions

```{r}
library(tidyverse)
library(AppliedPredictiveModeling)
library(igraph)
library(skimr)
library(broom)
library(yardstick)
library(modelr)
library(rsample)
library(recipes)
data(AlzheimerDisease) #Data to use
predictors %>% 
  cbind(diagnosis) ->
  alzheimers #Combine columns from the predictors and diagnosis tables

#DataExplorer::create_report(alzheimers, "alzheimers.html")

#From the report we can see that there are some weird distributions, and some correlations that we might want to look into

alzheimers %>% 
  ggplot(aes(x=Thyroid_Stimulating_Hormone)) +
  geom_histogram()

#alzheimers %>% 
  #filter(Thyroid_Stimulating_Hormone == min(Thyroid_Stimulating_Hormone)) %>% 
  #skimr::skim() # Look at the histograms for the variables with the rows where the value is extremely low, to see if there are any other weird distributions for these

alzheimers %>% 
  mutate(male = factor(male), #1
         Genotype = fct_infreq(fct_lump(Genotype, n=3))) -> #2
  alzheimers
#1 Turn into categorical rather than numeric
#2 fct_lump, lumps together the last categorical (n=3 means that you are not lumping together the first 3), fct_infreq changes from ordered alphabetically to most frequent to less frequent. This new cleaned data becomes the new dataset.

#Preparation to be able to create a model
#Splitting the data into sampledata with a training set and a testing set, using rsample

alzheimers %>% 
  initial_split(prop=.9) ->
  alz_split

alz_split %>% 
  training() ->
  alz_train

alz_split %>% 
  testing() ->
  alz_test

#The different variables are in very different scales, some are even negative, means that it is difficult to compare them, minmax-scoring (scale between 0-1), z-score (scale of -10 to +10, standard deviations). Using recepies. The following: dont touch diagnosis! Alz_processes will be diagnosis predicted by everything else.

alz_train %>% 
  recipe(diagnosis ~ .) %>% 
  step_center(all_numeric()) %>% #1
  step_scale(all_numeric()) %>%  #2
  prep(training=alz_train) -> #3
alz_preprocess 

#1 step_center says how far away from the mean they are, only on the numeric columns, the other once doesnt make sense
#2 step_Scale changes to z-scores, devides the means by the standard, only numeric
#3 depending on that data, what should the scaling look like, wont train until you do the prep

#We still have 130 numeric values - since we only have twice as many rows that is way too many
#We will use step_corr from recepies package - high correlation filter. 
# Other fun functions from recipes: There are also functions for filling in missing values, collapse some categorical values (like the fct_lump), shuffle, filter, and much more! step_*-functions.

alz_preprocess %>% 
  step_corr(all_numeric()) %>% #1
  step_nzv(all_predictors()) %>% #2
  step_zv(all_predictors()) %>% #3 
  step_pca(all_numeric()) %>% #4 
  step_upsample(diagnosis) %>% 
  prep(training=alz_train, retain=TRUE) ->
  alz_preprocess
  
  #1 Looks for variables that have a high correlation. Keeps the one in the correlation pair that have fewest correlations above a certain value to other variables
#2 Delete the once with near zero variance. Where there is one very, very dominant variable
#3 Delete variables with zero variance
#4 principle component analysis - chooses the pc:s that are most responsible for the differences - has most effect on the thing we are trying to predict
  
alz_preprocess %>% 
  bake(alz_train) -> #to apply it to the data in alz_train
  alz_train_p

alz_preprocess %>% 
  juice(all_outcomes(), all_predictors()) -> 
  alz_test_p

alz_preprocess %>% 
  bake(alz_test) -> #to apply it to the data in alz_test
  alz_test_p

#Building the classification model, glm-function, general linear model

rm("diagnosis")

alz_train_p %>% 
  glm(diagnosis~ ., data=., family = "binomial") ->
  alz_glm_full #family - what is the distribution family we are using, its binomial because we only have two descrete values, control or impaired

alz_glm_full %>%
  broom::glance() 

#when comparing models fitted by maximum likelihood of the same data, pick the one with smallest AIC and BIC

#Information about how the model is built:

alz_glm_full %>% 
  broom::tidy()

alz_glm_full %>% 
  broom::augment() %>% 
  View()

#Score model using the test data

alz_test_p %>% 
  modelr::add_predictions(alz_glm_full) %>% 
  mutate(class = factor(ifelse(pred >=0, "Control", "Impaired"), levels = c("Impaired","Control"))) ->
  alz_test_scored 

alz_test_scored %>%  View()

alz_test_scored %>% 
  yardstick::conf_mat(diagnosis, class) #Ger en false-positive etc matris

alz_test_scored %>% 
  yardstick::accuracy(diagnosis, class) #Testa accuracy - the number of correctly categorized divided by the total number 

alz_test_scored %>% 
  yardstick::spec(diagnosis, class)#the once that you get right that are positive

alz_test_scored %>%
  yardstick::sens(diagnosis, class) #the once that you get right that are negative
```

